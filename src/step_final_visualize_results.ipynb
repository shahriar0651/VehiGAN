{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice\n",
    "\n",
    "This file is currently under construction and will be updated soon. Please check back later for the finalized content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import yaml \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Date\n",
    "sysname = \"VehiGAN\"\n",
    "version = 'tcps_november'\n",
    "\n",
    "gen_size = 1000\n",
    "data_dir = Path(f'../artifacts/results_{version}/')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(open('../config/config.yaml'))\n",
    "attack_index_dict = {attack : indx for indx, attack in enumerate(cfg['selected_attacks'])}\n",
    "if not os.path.exists(\"../artifacts/plots/\"): \n",
    "\tos.makedirs(\"../artifacts/plots/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "advFnc = 'fgsm'\n",
    "per_files = glob.glob(f'../artifacts/results_{version}/advAttacks/adv_performance_*.csv')\n",
    "print(per_files)\n",
    "performance_df = pd.DataFrame([])\n",
    "for per_file_name in per_files:\n",
    "    if advFnc not in per_file_name:\n",
    "        continue\n",
    "    performance_df = pd.concat([performance_df, pd.read_csv(per_file_name, index_col=0)], axis=0)\n",
    "performance_df = performance_df.drop(columns=['support'])\n",
    "performance_df = performance_df.replace(advFnc, advFnc.upper())\n",
    "performance_df\n",
    "# performance_df = performance_df.replace(\"fgsm\", 'FGSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters in a loop\n",
    "def filter_df(original_data, filter_rules = None):\n",
    "    filtered_data = original_data.copy()\n",
    "    if filter_rules== None:\n",
    "        return filtered_data\n",
    "    for column, values in filter_rules.items():\n",
    "        values = [values] if not isinstance(values, list) else values\n",
    "        filtered_data = filtered_data[filtered_data[column].isin(values)]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dict = {\n",
    "    'fp': 'FPR',\n",
    "    'fn' : 'FNR'}\n",
    "\n",
    "for advType, metric in attack_dict.items():\n",
    "    print(\"Attack: \", advType)\n",
    "    filter_rules = {\n",
    "        'advType' : advType,\n",
    "        'advCap': 'indv'\n",
    "    }\n",
    "    data = filter_df(performance_df, filter_rules)\n",
    "    plt.figure(figsize=(3.75,2.5))\n",
    "    if data.shape[0] > 0:\n",
    "        sns.lineplot(data, x = 'Epsilon', y = metric, hue = 'advFunc',style = 'advFunc', markers=True, markersize = '10')\n",
    "        # sns.boxenplot(data, x = 'Epsilon', y = metric, hue = 'advFunc',)\n",
    "    # plt.legend(title='Attack Type', loc = 'upper left', framealpha=0.5, bbox_to_anchor = (0,1.2))\n",
    "    plt.legend( loc = 'upper left', framealpha=0.25, bbox_to_anchor = (0,1.0))\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../artifacts/plots/single_WGAN_{metric}.jpg\", dpi = 350)\n",
    "    plt.savefig(f\"../artifacts/plots/single_WGAN_{metric}.pdf\", dpi = 350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dict = {\n",
    "    'fp': 'FPR'}\n",
    "\n",
    "for advType, metric in attack_dict.items():\n",
    "    filter_rules = {\n",
    "        'advType' : advType,\n",
    "        'advCap': 'trans',\n",
    "        'advFunc': advFnc.upper()\n",
    "    }\n",
    "    data = filter_df(performance_df, filter_rules)\n",
    "    plt.figure(figsize=(3.75,2.5))\n",
    "    if data.shape[0] > 0:\n",
    "        sns.lineplot(data, x = 'Epsilon', y = metric, hue = 'modAcc',style = 'modAcc', markers=True, markersize='10')\n",
    "    plt.legend(title='Model Access', loc = 'upper left', framealpha=0.25)\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../artifacts/plots/trans_WGAN_{metric}.jpg\", dpi = 350)\n",
    "    plt.savefig(f\"../artifacts/plots/trans_WGAN_{metric}.pdf\", dpi = 350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize adversarial attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "per_files = glob.glob(f'../artifacts/FGSM_samples/*.txt')\n",
    "print(per_files)\n",
    "adv_Data = {}\n",
    "file_id_list = []\n",
    "for indx, per_file_name in enumerate(per_files):\n",
    "    file_id = Path(per_file_name).name.split(\"_\")[0]\n",
    "    file_id_list.append(file_id)\n",
    "    data_type = Path(per_file_name).name.split(\"_\")[-1].split(\".\")[0]\n",
    "    adv_Data[f\"{data_type}_{file_id}\"]= np.loadtxt(per_file_name)\n",
    "file_id_list = list(set(file_id_list))\n",
    "adv_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(open('../config/config.yaml'))\n",
    "from types import SimpleNamespace\n",
    "cfg = SimpleNamespace(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    '$v_x$',\n",
    "    '$\\Delta x$',\n",
    "    '$v_y$',\n",
    "    '$\\Delta y$',\n",
    "    '$a_x$',\n",
    "    '$\\Delta v_x$',\n",
    "    '$a_y$',\n",
    "    '$\\Delta a_y$',\n",
    "    '$\\Delta \\\\theta_x$',\n",
    "    '$\\omega_x$',\n",
    "    '$\\Delta \\\\theta_y$',\n",
    "    '$\\omega_y$']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sorted = [\n",
    "    '$\\Delta x$',\n",
    "    '$\\Delta y$',\n",
    "    '$v_x$',\n",
    "    '$v_y$',\n",
    "    '$\\Delta v_x$',\n",
    "    '$\\Delta a_y$',\n",
    "    '$a_x$',\n",
    "    '$a_y$',\n",
    "    '$\\Delta \\\\theta_x$',\n",
    "    '$\\Delta \\\\theta_y$',\n",
    "    '$\\omega_x$',\n",
    "    '$\\omega_y$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indx = 3\n",
    "for indx in range(0,1):\n",
    "\n",
    "    # Assuming indx, adv_Data, file_id_list, and cfg are defined\n",
    "    # Extract dataframes\n",
    "    df_ben = pd.DataFrame(adv_Data[f\"orgimage_{file_id_list[indx]}\"], columns=features, index=range(1,11))\n",
    "    df_adv = pd.DataFrame(adv_Data[f\"advimage_{file_id_list[indx]}\"], columns=features, index=range(1,11))\n",
    "    df_grad = pd.DataFrame(adv_Data[f\"gradient_{file_id_list[indx]}\"], columns=features, index=range(1,11))\n",
    "    df_pert = pd.DataFrame(adv_Data[f\"perturbation_{file_id_list[indx]}\"], columns=features, index=range(1,11))\n",
    "\n",
    "    # Generate a palette with ten distinct colors\n",
    "    colors = sns.color_palette('tab10', n_colors=len(cfg.features))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', '#FFA07A', '#00CED1', '#8A2BE2', '#FFD700', '#00FF00']\n",
    "\n",
    "    # Plot each feature with a unique color\n",
    "    plt.figure(figsize=(3.5,3))\n",
    "    for i, feature in enumerate(features_sorted):\n",
    "        plt.plot(df_ben[feature], linestyle='-', linewidth=1, marker='s', markersize = 7.5, color=colors[i], markeredgecolor='black', label=feature)\n",
    "        plt.plot(df_adv[feature], linestyle='-.', linewidth=0.5, marker='s', markersize = 6, color=colors[i], markeredgecolor='red')\n",
    "\n",
    "    # Add labels and legend with a black border\n",
    "    legend = plt.legend(loc = 'center', bbox_to_anchor=(0.5, 0.7), ncols=3, framealpha=1.0, fontsize = 8.5)\n",
    "    legend.get_frame().set_linewidth(1)  # Set legend border width\n",
    "    legend.get_frame().set_edgecolor('black')  # Set legend border color\n",
    "    plt.xticks(range(1,11))\n",
    "    plt.xlabel(\"Time step\", fontsize= 12)\n",
    "    plt.ylabel(\"Value\", fontsize= 12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../artifacts/plots/adv_FP_attack.jpg\", dpi = 350)\n",
    "    plt.savefig(f\"../artifacts/plots/adv_FP_attack.pdf\", dpi = 350)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---------- Gradient ---------\n",
    "    plt.figure(figsize=(3.5,3))\n",
    "    sns.heatmap(df_grad[features_sorted].T, cmap=\"PiYG\", linewidths=0.1, linecolor='k')\n",
    "    plt.ylabel(\"Features\", fontsize= 12)\n",
    "    plt.xlabel(\"Time step\", fontsize= 12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../artifacts/plots/adv_FP_grad.jpg\", dpi = 350)\n",
    "    plt.savefig(f\"../artifacts/plots/adv_FP_grad.pdf\", dpi = 350)\n",
    "    plt.show()\n",
    "\n",
    "    # --------- benign_sample ---------\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(df_ben[features_sorted], cmap=\"PiYG\", linewidths=0.1, linecolor='k', vmin=0, vmax=1.0, cbar=False)\n",
    "    plt.xlabel(\"Features\", fontsize= 12)\n",
    "    plt.ylabel(\"Time step\", fontsize= 12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../artifacts/plots/benign_sample_{indx}.jpg\", dpi = 350)\n",
    "    plt.savefig(f\"../artifacts/plots/benign_sample_{indx}.pdf\", dpi = 350)\n",
    "    plt.show()\n",
    "\n",
    "    # #-------------- cropped benign_sample -----------------\n",
    "    # plt.figure(figsize=(3,3))\n",
    "    # sns.heatmap(df_ben[features_sorted], cmap=\"PiYG\", linewidths=0.1, linecolor='k', vmin=0, vmax=1.0, cbar=False, \n",
    "    #             xticklabels=False, yticklabels=False)  # Remove ticks and labels\n",
    "\n",
    "    # plt.gca().set_axis_off()  # Hide axis lines\n",
    "    # plt.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Remove whitespace around the figure\n",
    "    # plt.savefig(f\"../artifacts/plots/croped_benign_sample_{indx}.jpg\", dpi=350, bbox_inches='tight', pad_inches=0)\n",
    "    # plt.savefig(f\"../artifacts/plots/croped_benign_sample_{indx}.pdf\", dpi=350, bbox_inches='tight', pad_inches=0)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Example 2D NumPy matrix\n",
    "# matrix =df_grad.values\n",
    "\n",
    "# # Create 3D bar plot with matplotlib\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Get matrix shape\n",
    "# x_size, y_size = matrix.shape\n",
    "# x_positions, y_positions = np.meshgrid(np.arange(x_size), np.arange(y_size))\n",
    "\n",
    "# # Flatten matrices and convert to 1D arrays\n",
    "# x_positions = x_positions.flatten()\n",
    "# y_positions = y_positions.flatten()\n",
    "# z_values = matrix.flatten()\n",
    "\n",
    "# # Create 3D bars\n",
    "# ax.bar3d(x_positions, y_positions, np.zeros_like(z_values), 1, 1, z_values,)\n",
    "\n",
    "# # Add labels and title\n",
    "# ax.set_xlabel('X Axis')\n",
    "# ax.set_ylabel('Y Axis')\n",
    "# ax.set_zlabel('Values')\n",
    "# ax.set_title('3D Bar Plot from NumPy Matrix')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ben.plot(figsize=(10,3), marker = 'p')\n",
    "# df_adv.plot(figsize=(10,3), marker = 'x')\n",
    "# (df_ben - df_adv).plot(marker='p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_rules = {\n",
    "#     'advFunc': 'fgsm'\n",
    "# }\n",
    "# data = filter_df(performance_df, filter_rules)\n",
    "# if data.shape[0] > 0:\n",
    "#     sns.set_style(\"ticks\",{'axes.grid' : True})\n",
    "#     ax = sns.catplot(data, col= 'advType', x = 'Epsilon', y = 'FPR', kind='point', dodge=True, height=3, aspect=1.25)\n",
    "\n",
    "\n",
    "# filter_rules = {\n",
    "#     'advFunc': 'fgsm'\n",
    "# }\n",
    "# data = filter_df(performance_df, filter_rules)\n",
    "# if data.shape[0] > 0:\n",
    "#     sns.set_style(\"ticks\",{'axes.grid' : True})\n",
    "#     ax = sns.catplot(data, col= 'advType', x = 'Epsilon', y = 'FPR', hue = 'modAcc', kind='point', dodge=True, height=3, aspect=1.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Robust Ensemble Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benign Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "per_files = glob.glob(f'../artifacts/results_{version}/ens_detector_*_mean_10_benign_indv_0.01.csv')\n",
    "print(per_files)\n",
    "performance_dict = {}\n",
    "for per_file_name in per_files:\n",
    "    metric, mean, window, eval_type = Path(per_file_name.replace('.csv', '')).name.split(\"_\")[2:-2]\n",
    "    performance_dict[f\"{metric}_{mean}_{window}_{eval_type}\"] = pd.read_csv(per_file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Where to find the Tables\n",
    "for i, (key, value) in enumerate(performance_dict.items()):\n",
    "    plt.figure(figsize=(5.25,3))\n",
    "    # Set the colormap to go from green to red\n",
    "    custom_cmap = sns.color_palette(\"RdYlGn\", as_cmap=True)\n",
    "    # if i == len(performance_dict)-1:\n",
    "    #    cbar = True\n",
    "    # else:\n",
    "    # annot_kws={'fontsize': 12, 'fontstyle': 'italic', 'color':'b', 'alpha': 0.6,\n",
    "    #                    'rotation': 'vertical', 'verticalalignment': 'center', 'backgroundcolor': 'w'}\n",
    "    annot_kws={'color':'k', 'verticalalignment': 'center'}\n",
    "    \n",
    "    cbar = True #vmin = 0.80, vmax = 0.90,\n",
    "    ax = sns.heatmap(value, annot=True, fmt = \".2f\", annot_kws=annot_kws,  cmap=custom_cmap, linewidths=0.5, alpha=0.5, linecolor='gray', square=False, cbar=cbar)\n",
    "    for _, spine in ax.spines.items():\n",
    "      spine.set_visible(True)\n",
    "    # plt.title(key)\n",
    "    plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n",
    "    plt.xlabel(\"Number of candidate models, $m$\")\n",
    "    plt.ylabel(\"Number of deployed models, $k$\")\n",
    "    ax.xaxis.tick_top() # x axis on top\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.tight_layout()\n",
    "    file_dir = f'../artifacts/plots/robust_detector_{key}.jpg'\n",
    "    file_dir = f'../artifacts/plots/robust_detector_{key}.pdf'\n",
    "    plt.savefig(file_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "per_files = glob.glob(f'../artifacts/results_{version}/advAttacks/ens_detector_fpr_mean_*.csv')\n",
    "print(per_files)\n",
    "performance_dict = {}\n",
    "for per_file_name in per_files:\n",
    "    window, advType, eps = Path(per_file_name.replace('.csv', '')).name.split(\"_\")[-3:]\n",
    "    performance_dict[f\"Setting: {window}, {advType}, {eps}\"] = pd.read_csv(per_file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (key, value) in enumerate(performance_dict.items()):\n",
    "#     plt.figure(figsize=(5.25,3))\n",
    "#     # Set the colormap to go from green to red\n",
    "#     custom_cmap = sns.color_palette(\"RdYlGn_r\", as_cmap=True)\n",
    "#     # if i == len(performance_dict)-1:\n",
    "#     #    cbar = True\n",
    "#     # else:\n",
    "#     cbar = True\n",
    "#     annot_kws={'color':'k', 'verticalalignment': 'center'}\n",
    "  \n",
    "#     ax = sns.heatmap(value, annot=True, fmt = \".2f\", vmin=0, vmax=1, annot_kws=annot_kws, cmap=custom_cmap, linewidths=0.5, alpha=0.5, linecolor='gray', square=False, cbar=cbar)\n",
    "#     for _, spine in ax.spines.items():\n",
    "#       spine.set_visible(True)\n",
    "#     # plt.title(key) \n",
    "#     plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n",
    "#     plt.xlabel(\"Number of candidate models, $m$\")\n",
    "#     plt.ylabel(\"Number of deployed models, $k$\")\n",
    "#     ax.xaxis.tick_top() # x axis on top\n",
    "#     ax.xaxis.set_label_position('top')\n",
    "#     plt.tight_layout()\n",
    "#     file_dir = f'../artifacts/plots/robust_detector_{key}.jpg'\n",
    "#     file_dir = f'../artifacts/plots/robust_detector_{key}.pdf'\n",
    "#     plt.savefig(file_dir)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key, value) in enumerate(performance_dict.items()):\n",
    "    print(key)\n",
    "    # plt.figure(figsize=(5.25,3))\n",
    "    # Set the colormap to go from green to red\n",
    "    custom_cmap = sns.color_palette(\"RdYlGn_r\", as_cmap=True)\n",
    "    # if i == len(performance_dict)-1:\n",
    "    #    cbar = True\n",
    "    # else:\n",
    "    cbar = True\n",
    "    fig, axes = plt.subplots(1,2, figsize=(5.5,2.5), sharex=True, sharey = True)\n",
    "\n",
    "    ax = axes[0]\n",
    "    sns.boxplot(value, color='green', boxprops=dict(alpha=.50), ax = ax)\n",
    "    sns.lineplot(value.mean(axis=0), marker = 'p', color = 'k', markersize = '8', ax = ax)\n",
    "    ax.set_xlabel(\"# of candidate models, $m$\")\n",
    "    ax.set_ylabel(\"FPR\")\n",
    "    ax.grid(True)\n",
    "    ax = axes[1]\n",
    "    sns.lineplot(value.T.iloc[:,:6], linestyle='-', markers=True, markersize= '8', ax = ax)\n",
    "    ax.set_xlabel(\"# of candidate models, $m$\")\n",
    "    ax.set_ylabel(\"FPR\")\n",
    "    ax.legend(title='# of deployed models, $k$', ncols = 2, framealpha = 0.50)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    file_dir = f'../artifacts/plots/robust_detector_{key}_new.jpg'\n",
    "    file_dir = f'../artifacts/plots/robust_detector_{key}_new.pdf'\n",
    "    plt.savefig(file_dir)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load scalability data\n",
    "# import pandas as pd\n",
    "# df_scale_1 = pd.read_csv(\"../artifacts/plots/data_analysis/scalability_inference_comp_False.csv\", index_col=0)\n",
    "# # df_scale_2 = pd.read_csv(\"../artifacts/plots/data_analysis/scalability_inference_comp_True.csv\", index_col=0)\n",
    "# # df_scale = pd.concat([df_scale_1, df_scale_2], axis = 0)\n",
    "# df_scale = pd.concat([df_scale_1], axis = 0)\n",
    "# df_scale[['GANType', '# Layers', 'Latent Dim', 'Training Epoch', 'Dummy']] = df_scale['Model'].str.split(\"_\", expand=True)\n",
    "# df_scale['# Layers'] = df_scale['# Layers'].astype(int)\n",
    "# df_scale['# Layers']+= 3\n",
    "# df_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from matplotlib import pylab as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # sns.barplot(df_scale, y = 'Inference Time', x=\"Model\",)\n",
    "# # sns.swarmplot(df_scale, y = 'Inference Time', x=\"Model\", hue=\"Model\", alpha = 0.75)\n",
    "# # # sns.violinplot(df_scale, y = 'Inference Time', x=\"Model Type\",)\n",
    "# # plt.legend([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = df_scale[df_scale['Model Type'] == 'Standard (CPU)']\n",
    "# data = data.reset_index()\n",
    "# data['Model Index'] = data.index + 1\n",
    "# plt.figure(figsize=(2.75,1.85))\n",
    "# # sns.scatterplot(data, y = 'Inference Time', x=\"Model No\",)\n",
    "# # sns.lineplot(data, y = 'Inference Time', x=\"Model Index\", hue = '# Layers',  marker = 'o',\n",
    "# #              linewidth = '0.35', markersize = '3.5',\n",
    "# #             markeredgecolor='none')\n",
    "# sns.lineplot(data, y = 'Inference Time', x=\"Model Index\", hue = '# Layers', style = '# Layers', markers =True,\n",
    "#              linewidth = '0.35', markersize = '3.5',\n",
    "#             markeredgecolor='none', palette=sns.color_palette(\"tab10\"))\n",
    "# plt.grid(True)\n",
    "# plt.ylim(0)\n",
    "# plt.ylabel(\"Inference Time (ms)\")\n",
    "# plt.legend(title = '# Layers', loc = 'center', bbox_to_anchor = (0.80, 0.40))\n",
    "# # plt.xticks(range(1, 60, 10))\n",
    "# plt.tight_layout()\n",
    "# file_dir = f'../artifacts/plots/scalability_std.jpg'\n",
    "# file_dir = f'../artifacts/plots/scalability_std.pdf'\n",
    "# plt.savefig(file_dir)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# data = df_scale[df_scale['Model Type'] == 'TFLite (CPU)']\n",
    "# import numpy as np\n",
    "# data = data.reset_index()\n",
    "# data['Model Index'] = data.index + 1\n",
    "# plt.figure(figsize=(2.75,1.85))\n",
    "# # sns.scatterplot(data, y = 'Inference Time', x=\"Model No\",)\n",
    "# sns.lineplot(data, y = 'Inference Time', x=\"Model Index\", hue = '# Layers', style = '# Layers', markers =True,\n",
    "#              linewidth = '0.35', markersize = '3.5',\n",
    "#             markeredgecolor='none', palette=sns.color_palette(\"tab10\")\n",
    "# )\n",
    "# plt.legend(title = '# Layers', loc = 'center', bbox_to_anchor = (0.80, 0.40))\n",
    "# plt.grid(True)\n",
    "# plt.ylim(0)\n",
    "# plt.ylabel(\"Inference Time (ms)\")\n",
    "# plt.tight_layout()\n",
    "# file_dir = f'../artifacts/plots/scalability_lite.jpg'\n",
    "# file_dir = f'../artifacts/plots/scalability_lite.pdf'\n",
    "# plt.savefig(file_dir)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from matplotlib import pylab as plt\n",
    "# sns.barplot(df_scale, y = 'Inference Time', x=\"Model Type\",)\n",
    "# sns.swarmplot(df_scale, y = 'Inference Time', x=\"Model\", hue=\"Model\", alpha = 0.75)\n",
    "# # sns.violinplot(df_scale, y = 'Inference Time', x=\"Model Type\",)\n",
    "# plt.legend([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Different Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "per_files = glob.glob(f'../artifacts/results_{version}/performance_gen_dis*.csv')\n",
    "print(per_files)\n",
    "performance_df = pd.DataFrame([])\n",
    "for per_file_name in per_files:\n",
    "    performance_df = pd.concat([performance_df, pd.read_csv(per_file_name, index_col=0)], axis=0)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in performance_df.index:\n",
    "    _, window, no_of_layers, latent_dim, epochs, _ = model_id.split(\"_\")\n",
    "    performance_df.loc[model_id, \"window\"] = int(window)\n",
    "    performance_df.loc[model_id, 'no_of_layers'] = int(no_of_layers)\n",
    "    performance_df.loc[model_id, 'latent_dim'] = int(latent_dim)\n",
    "    performance_df.loc[model_id, 'epochs'] = int(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = performance_df.sort_values([\"AUROC\"],ascending=False)\n",
    "performance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hue in ['no_of_layers', 'latent_dim', 'epochs']:\n",
    "    plt.figure()\n",
    "    sns.boxenplot(performance_df, x = hue, y = \"AUROC\", hue =hue, palette = 'Blues', alpha = 0.50) # x = \"window\"\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(performance_df, x = \"window\", y = \"AUROC\", hue = \"latent_dim\", palette = 'Blues', alpha = 0.50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(performance_df, x = \"window\", y = \"AUROC\", hue = \"latent_dim\", palette = 'Blues', alpha = 0.50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(performance_df[performance_df[\"window\"] == 10], x = \"epochs\", y = \"AUROC\", hue = 'no_of_layers',  palette = 'Blues', alpha = 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather all Data for WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GS for all the generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_files = glob.glob(f'../artifacts/results_{version}/gen*.json')\n",
    "\n",
    "gen_eval_df = pd.DataFrame([])\n",
    "for gen_file_name in gen_files:\n",
    "    if f\"wgan_{window}\" not in gen_file_name:\n",
    "        continue\n",
    "    model_id = \"_\".join( Path(gen_file_name).name.split(\"_\")[1:-1])\n",
    "    with open(f\"{gen_file_name}\", \"r\") as fp: \n",
    "        gen_score = json.load(fp)\n",
    "    gen_eval_df[model_id] = pd.Series(gen_score)\n",
    "gen_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DS for all the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "dis_files = glob.glob(f'../artifacts/results_{version}/dis*.json')\n",
    "\n",
    "dis_eval_df = pd.DataFrame([])\n",
    "for dis_file_name in dis_files:\n",
    "    if f\"wgan_{window}\" not in dis_file_name:\n",
    "        continue\n",
    "    model_id = \"_\".join( Path(dis_file_name).name.split(\"_\")[1:-1])\n",
    "    with open(f\"{dis_file_name}\", \"r\") as fp: \n",
    "        dis_score = json.load(fp)\n",
    "    dis_score = pd.DataFrame(dis_score).T\n",
    "    dis_score['mAUC'] = dis_score[['AUROC', 'AUPRC']].mean(axis=1)\n",
    "    dis_eval_df[model_id] = dis_score['AUROC'] #TODO: Change here for another DS metric\n",
    "dis_eval_df = dis_eval_df.T\n",
    "indeces = [\"_\".join(ind.split(\"_\")[0:-1]) for ind in dis_eval_df.index]\n",
    "dis_eval_df.index  = indeces\n",
    "dis_eval_df = dis_eval_df.dropna() #TODO: Evaluate the metrics again (Nan)\n",
    "dis_eval_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Score by Max Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_ind_score = pd.DataFrame([])\n",
    "for dis_file_name in dis_files:\n",
    "    model_id = \"_\".join( Path(dis_file_name).name.split(\"_\")[1:-1])\n",
    "    with open(f\"{dis_file_name}\", \"r\") as fp: \n",
    "        dis_score = json.load(fp)\n",
    "    dis_score = pd.DataFrame(dis_score).T\n",
    "    dis_score['mAUC'] = dis_score[['AUROC', 'AUPRC']].mean(axis=1)\n",
    "    _, window, no_of_layers, latent_dim, epochs, _ = model_id.split(\"_\")\n",
    "    dis_score[\"window\"] = int(window)\n",
    "    dis_score['no_of_layers'] = int(no_of_layers)\n",
    "    dis_score['latent_dim'] = int(latent_dim)\n",
    "    dis_score['epochs'] = int(epochs)\n",
    "    param = model_id.split(\"_\")[0:-1]\n",
    "    dis_score['Model']  = \"_\".join(param)\n",
    "    params = f\"$window$={param[1]}, $layers$={param[2]}, $noise~dim$={param[3]}, $epoch$={param[4]}\"\n",
    "    dis_score['WGAN Parameter'] = params\n",
    "    dis_ind_score = pd.concat([dis_ind_score, dis_score], axis= 0, ignore_index=True)\n",
    "\n",
    "dis_ind_score_max = dis_ind_score.groupby('Attack').max()\n",
    "dis_ind_score_max['Attack'] = dis_ind_score_max.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing attack name with attack index\n",
    "dis_eval_df.rename(columns=attack_index_dict, inplace=True)\n",
    "dis_ind_score = dis_ind_score.replace(attack_index_dict)\n",
    "# dis_ind_score_max = dis_ind_score_max.replace(attack_index_dict)\n",
    "dis_ind_score_max.rename(index=attack_index_dict, inplace=True)\n",
    "# dis_eval_df = dis_eval_df.astype(float).round(decimals = 4)\n",
    "# dis_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top k models\n",
    "top_models_df = dis_eval_df.mean(axis = 1).sort_values(ascending=False)\n",
    "top_models = []\n",
    "for model_id in top_models_df.index:\n",
    "    candidate = model_id.split(\"_\")[1:-1]\n",
    "    if candidate not in top_models:\n",
    "        top_models.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(top_models_df.astype(float), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stability of top 2 WGAN model (based of ADS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Stability of top 2 WGAN model (based of ADS)\n",
    "fig, axes = plt.subplots(2, 1, figsize= (5, 3.5), sharex=True)\n",
    "\n",
    "metric = 'AUROC'\n",
    "palette = sns.color_palette(\"mako_r\", 6)\n",
    "\n",
    "for ax, (window, no_of_layers, latent_dim)  in zip(axes, top_models):\n",
    "    dis_ind_score_cut =  dis_ind_score[dis_ind_score['no_of_layers'] == int(no_of_layers)]\n",
    "    dis_ind_score_cut =  dis_ind_score_cut[dis_ind_score_cut['latent_dim'] == int(latent_dim)]\n",
    "    dis_ind_score_cut =  dis_ind_score_cut[dis_ind_score_cut['window'] == int(window)]\n",
    "\n",
    "    # Selecting four attacks to visualize based on max change in AUROC\n",
    "    attack_slope = {}\n",
    "    for attack in dis_ind_score_cut[\"Attack\"].unique():\n",
    "        score = dis_ind_score_cut[dis_ind_score_cut[\"Attack\"] == attack]['mAUC'].diff().mean()\n",
    "        attack_slope[attack] = score\n",
    "    sorted_attacks = list(pd.Series(attack_slope).sort_values().index)\n",
    "    top_attacks = sorted_attacks[0:2] + sorted_attacks[-2:]\n",
    "    dis_ind_score_cut  = dis_ind_score_cut[dis_ind_score_cut['Attack'].isin(top_attacks)]\n",
    "\n",
    "    # TODO: Change mAUC to AUROC\n",
    "    sns.set_palette(\"PuBuGn_d\")\n",
    "    ax = sns.lineplot(dis_ind_score_cut, hue = 'Attack', style = 'Attack', linestyle = '--'\n",
    "                 , y = metric, x = 'epochs', linewidth = '0.75'\n",
    "                 , markersize = '8', dashes=False,  markers = True, palette=\"tab10\", ax = ax) #marker = 'p', markers = True, \n",
    "\n",
    "    ax.legend(title = \"Attack Index: \", bbox_to_anchor = (1.25,1.075), loc='upper center', fontsize = '10')\n",
    "    ax.set_title(f\"A Single WGAN with $window$={window} $layers$={no_of_layers}, $noise~dim$={latent_dim}\", fontsize = '10')\n",
    "    ax.grid(True)\n",
    "    ax.set_ylim([0.0, 1.1])\n",
    "\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/multiple_attacks_{metric}_WGAN_{window}.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/multiple_attacks_{metric}_WGAN_{window}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of top three WGAN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize= (10, 2),sharex=True)\n",
    "mask = dis_ind_score['Model'].isin(list(top_models_df.index[0:3]))\n",
    "sns.lineplot(dis_ind_score_max.replace(attack_index_dict).sort_values([metric], ascending=False), x = 'Attack', y = metric,  label = \"Upper-bound\", color = 'k',markersize = '7', marker=\"p\", ax = ax)\n",
    "sns.scatterplot(dis_ind_score, x = 'Attack', y = metric, hue = 'Model',  markers=True, palette=sns.mpl_palette(\"Set2\"), legend=None, color = 'k', alpha = 0.20, ax = ax)\n",
    "sns.lineplot(dis_ind_score[mask], x = 'Attack', y =metric, hue = 'WGAN Parameter', style = 'WGAN Parameter', linewidth = '1', markers = True, palette=sns.mpl_palette(\"Set2\"), markersize = '8', ax = ax)\n",
    "plt.yticks(np.array(range(0, 110, 20))/100)\n",
    "# ax.legend(title = \"Top Three WGAN Models with:\", bbox_to_anchor = (0.4, 0.185), loc = 'center', ncols = 2, framealpha=0.85, fontsize=8.5) #bbox_to_anchor = (0.3, 0.20),\n",
    "ax.legend(bbox_to_anchor = (0.475, 0.20), loc = 'center', ncols = 2, framealpha=0.65, fontsize=8.5) #bbox_to_anchor = (0.3, 0.20),\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Attack Index\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/max_scores_top_score_all_attacks_WGAN_{window}.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/max_scores_top_score_all_attacks_WGAN_{window}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble-based VeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load correlation coefficient between G & D\n",
    "window = 10\n",
    "corr_gen_dis_dir = Path(f'{data_dir}/correlation_gen_dis_{window}.csv')\n",
    "corr_gen_dis = pd.read_csv(corr_gen_dis_dir, index_col=0)\n",
    "col_rename = {\"W_Distance\" : \"WD\", \"FID_Score\" : \"FID\", \"KID_Score\" : \"KID\"}\n",
    "corr_gen_dis.rename(columns = col_rename, index=col_rename, inplace=True)\n",
    "corr_gen_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_selected = ['SS_TRTS_Score',\n",
    "       'SS_TSTR_Score',  'TS_MMD_Score', 'TS_FD_Score',\n",
    "       'TS_KS_Score', 'TS_AD_Score', 'TS_WMD_Score',\n",
    "       'IM_FID_Score', 'IM_KID_Score']\n",
    "\n",
    "columns_selected_rename = {'SS_TRTS_Score' : 'TRTS',\n",
    "       'SS_TSTR_Score' : 'TSTR',  'TS_MMD_Score': 'MMD', 'TS_FD_Score' : 'FD',\n",
    "       'TS_KS_Score' : 'KS', 'TS_AD_Score' : 'AD', 'TS_WMD_Score' : 'EMD',\n",
    "       'IM_W_Distance' : 'WD', 'IM_FID_Score' : 'FID', 'IM_KID_Score' : 'KID'}\n",
    "\n",
    "columns_selected_new = [columns_selected_rename[col] for col in columns_selected]\n",
    "\n",
    "corr_gen_dis = corr_gen_dis.rename(columns=columns_selected_rename, index=columns_selected_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_gen_dis['AD'] = - corr_gen_dis['AD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.5,2.5))\n",
    "sns.heatmap(np.abs(corr_gen_dis.loc[columns_selected_new,columns_selected_new]), vmin=-1,vmax=1, \n",
    "            annot=True, \n",
    "            fmt=\"0.2f\", \n",
    "            cmap=\"RdYlGn\", \n",
    "            annot_kws={\"size\": 6}, \n",
    "            linewidths=0.05, \n",
    "            linecolor = 'black')\n",
    "# plt.title(\"Correlation among AUROC\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/Correlation_coeff_dis_gen_{window}.pdf\", dpi = 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2.5,2.5))\n",
    "coff_auroc = corr_gen_dis['AUROC']*(-1)\n",
    "coff_auroc\n",
    "np.abs(coff_auroc[columns_selected_new]).plot.bar()\n",
    "# plt.title(\"Correlation with AUROC\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/Correlation_auroc_dis_gen_{window}.pdf\", dpi = 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coff_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indeces = list(col_rename.values()) + ['AUROC', 'AUPRC']\n",
    "# plt.figure(figsize=(5,4))\n",
    "# sns.heatmap(corr_gen_dis.loc[indeces, indeces], cmap=\"PiYG\", linewidths='2', linecolor='k', annot=True, fmt='.3f',\n",
    "#             vmin=-1, vmax=1)\n",
    "# plt.tight_layout()\n",
    "# plt.title(f\"Correlation Heatmap with Window {window}\")\n",
    "# plt.savefig(f\"../artifacts/plots/correlation_heapmap_WGAN_{window}.jpg\", dpi = 350)\n",
    "# plt.savefig(f\"../artifacts/plots/correlation_heapmap_WGAN_{window}.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_perf_ads_dir = data_dir / f'ens_detector_ads_{window}.csv'\n",
    "ens_perf_ags_dir = data_dir / f'ens_detector_ags_{window}.csv'\n",
    "\n",
    "ens_perf_ags = pd.read_csv(ens_perf_ags_dir, index_col=0)\n",
    "# ens_perf_ags.replace(attack_index_dict, inplace=True)\n",
    "ens_perf_ags['mAUC'] = ens_perf_ags[['AUROC', 'AUPRC']].mean(axis=1)\n",
    "ens_perf_ags['Base'] = f'${sysname}_{{AGS}}$'\n",
    "\n",
    "ens_perf_ads = pd.read_csv(ens_perf_ads_dir, index_col=0)\n",
    "# ens_perf_ads.replace(attack_index_dict, inplace=True)\n",
    "ens_perf_ads['mAUC'] = ens_perf_ads[['AUROC', 'AUPRC']].mean(axis=1)\n",
    "ens_perf_ads['Base'] = f'${sysname}_{{ADS}}$'\n",
    "\n",
    "ens_perf_ads[\"Model\"] = [f\"$VehiGAN_{{ADS}}^{{{x}}}$\" for x in ens_perf_ads['Ensemble'].values]\n",
    "ens_perf_ags[\"Model\"] = [f\"$VehiGAN_{{AGS}}^{{{x}}}$\" for x in ens_perf_ags['Ensemble'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_perf_ags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_perf_ags[ens_perf_ags['Ensemble'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_perf_max_dir = data_dir / f'max_detector_{window}.csv'\n",
    "ens_perf_max = pd.read_csv(ens_perf_max_dir, index_col=0)\n",
    "ens_perf_max.replace(attack_index_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model for each ensemble \n",
    "perf_dis_ads_top = ens_perf_ads[['AUROC', 'AUPRC', 'mAUC','Ensemble']].groupby('Ensemble').mean().sort_values(metric, ascending=False).head(20)\n",
    "ads_max_count = perf_dis_ads_top.index[0]\n",
    "# ads_max_count = 5 #TODO: Overriding the model numbers\n",
    "\n",
    "perf_dis_ags_top = ens_perf_ags[['AUROC', 'AUPRC', 'mAUC','Ensemble']].groupby('Ensemble').mean().sort_values(metric, ascending=False).head(10)\n",
    "ags_max_count = perf_dis_ags_top.index[0]\n",
    "\n",
    "print(f\"Best model when {ads_max_count}, and {ags_max_count} models are ensembed in ADS, and AGS, respectively\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_perf_comb = pd.concat([ens_perf_ags, ens_perf_ads], axis = 0)\n",
    "ens_perf_comb['Model'] = [f\"{x[:-1]}^k$\" for x in ens_perf_comb['Base'].values]\n",
    "ens_perf_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of ensemble-based VeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'$VehiGAN_{AGS}^k$' : '$VehiGAN_m^k$ (G-based selection)',\n",
    "               '$VehiGAN_{ADS}^k$' : '$VehiGAN_m^k$ (D-based selection)',}\n",
    "ens_perf_comb = ens_perf_comb.replace(rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dis_cut = ens_perf_comb[ens_perf_comb['Ensemble'] <=10]\n",
    "plt.figure(figsize=(4, 2.5))\n",
    "sns.boxplot(perf_dis_cut, x = 'Ensemble', y = 'AUROC', hue = 'Model',  palette=sns.mpl_palette(\"Set2\"))\n",
    "plt.legend( ncols = 1, loc = 'center',  bbox_to_anchor = (0.50, 0.2), framealpha = 0.5, fontsize = '9') #title = 'VeGAN Model:',\n",
    "# plt.title(f\"WGAN-based Ensebmed Detector with Window = {window}\")\n",
    "plt.xlabel(\"Number of candidate/deployed WGANs, $m/k$\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/ensemble_techniques_compared_WGAN_{window}.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/ensemble_techniques_compared_WGAN_{window}.pdf\", dpi = 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dis_cut = ens_perf_comb[ens_perf_comb['Ensemble'] <=10]\n",
    "plt.figure(figsize=(4, 2.5))\n",
    "sns.lineplot(perf_dis_cut, x = 'Ensemble', y = 'AUROC', hue = 'Model',  palette=sns.mpl_palette(\"Set2\"), marker='p')\n",
    "plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "# plt.legend( ncols = 2, loc = 'center',  bbox_to_anchor = (0.50, 0.075), framealpha = 0.95) #title = 'VeGAN Model:',\n",
    "# plt.title(f\"WGAN-based Ensebmed Detector with Window = {window}\")\n",
    "plt.xlabel(\"Number of candidate/deployed WGANs, $m/k$\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/ensemble_techniques_compared_WGAN_{window}.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/ensemble_techniques_compared_WGAN_{window}.pdf\", dpi = 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, figsize = (5, 4),sharex=True)\n",
    "no_ensemble = 2\n",
    "sysname = \"VehiGAN\"\n",
    "metric = \"AUROC\"\n",
    "\n",
    "ax = axes[0]\n",
    "fil_1 = ens_perf_ads['Ensemble'] == 1 \n",
    "fil_2 = ens_perf_ads['Ensemble'] == ads_max_count \n",
    "perf_dis_cut = ens_perf_ads.where(fil_1 | fil_2).dropna()\n",
    "perf_dis_cut = perf_dis_cut.replace(attack_index_dict)\n",
    "perf_dis_cut = perf_dis_cut.sort_values('Model', ascending=True)\n",
    "sns.lineplot(perf_dis_cut.sort_values([metric], ascending=False), x = 'Attack', y = metric,  hue = 'Model',  style = 'Model', linewidth = '0.75', markers=True, markersize = '8.5', palette=sns.mpl_palette(\"Set2\"), ax = ax)\n",
    "sns.lineplot(ens_perf_max.sort_values([metric], ascending=False), x = 'Attack', y = metric,  label = \"Max\", color = 'k', marker=\"p\", linewidth = '0.5',  markersize = '5', ax = ax)\n",
    "ax.grid(True)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], bbox_to_anchor = (0.5,0.15), loc='center', framealpha = 0.85, ncols = 3)\n",
    "ax.set_title(f\"{sysname} with ADS-based Model Selection with Window = {window}\", fontsize = '10')\n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "fil_1 = ens_perf_ags['Ensemble'] == 1 \n",
    "fil_2 = ens_perf_ags['Ensemble'] == ags_max_count \n",
    "perf_dis_cut = ens_perf_ags.where(fil_1 | fil_2).dropna()\n",
    "perf_dis_cut = perf_dis_cut.replace(attack_index_dict)\n",
    "perf_dis_cut = perf_dis_cut.sort_values('Model', ascending=True)\n",
    "sns.lineplot(perf_dis_cut.sort_values([metric], ascending=False), x = 'Attack', y = metric,  hue = 'Model',  style = 'Model', linewidth = '0.75', markers=True, markersize = '8.5', palette=sns.mpl_palette(\"Set2\"), ax = ax)\n",
    "sns.lineplot(ens_perf_max.sort_values([metric], ascending=False), x = 'Attack', y = metric,  label = \"Max\", color = 'k', marker=\"p\", linewidth = '0.5',  markersize = '5', ax = ax)\n",
    "ax.grid(True)\n",
    "ax.set_title(f\"{sysname} with AGS-based Model Selection with Window = {window}\", fontsize = '10')\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], bbox_to_anchor = (0.5,0.15), loc='center', framealpha = 0.85, ncols = 3)\n",
    "ax.set_xlabel(\"Attack Index\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/improvements_over_ensemble_WGANs_{window}.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/improvements_over_ensemble_WGANs_{window}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get score for all the autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = 'artifacts'\n",
    "version = 'november_8_wisec_ae_8'\n",
    "# version = 'october_24_wisec_ae_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_files = glob.glob(f'../{artifacts}/results_{version}/ae*.json')\n",
    "ae_ind_score = pd.DataFrame([])\n",
    "ae_eval_df = pd.DataFrame([])\n",
    "# ae_ind_score = {}\n",
    "for ae_file_name in ae_files:\n",
    "    model_id = \"_\".join( Path(ae_file_name).name.split(\"_\")[1:-1]).split(\".\")[0]\n",
    "    with open(f\"{ae_file_name}\", \"r\") as fp: \n",
    "        ae_score = json.load(fp)\n",
    "    ae_score = pd.DataFrame(ae_score).T\n",
    "    ae_score['mAUC'] = ae_score[['AUPRC', 'AUPRC']].mean(axis=1)\n",
    "    ae_eval_df[model_id] = ae_score['AUROC']\n",
    "    # ae_ind_score[model_id] = ae_score\n",
    "\n",
    "    _, window, no_of_layers, epochs = model_id.split(\"_\")\n",
    "    ae_score[\"window\"] = int(window)\n",
    "    ae_score['no_of_layers'] = int(no_of_layers)\n",
    "    ae_score['latent_dim'] = int(0)\n",
    "    ae_score['epochs'] = int(epochs.split(\".\")[0])\n",
    "    ae_score['Model']  = model_id.split(\".\")[0]\n",
    "    ae_ind_score = pd.concat([ae_ind_score, ae_score], axis= 0, ignore_index=True)\n",
    "\n",
    "ae_ind_score_max = ae_ind_score.groupby('Attack').max()\n",
    "ae_ind_score_max['Attack'] = ae_ind_score_max.index\n",
    "ae_ind_score_max.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_ind_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_eval_df = ae_eval_df.T\n",
    "indeces = [\"_\".join(ind.split(\".\")[0].split(\"_\")) for ind in ae_eval_df.index]\n",
    "ae_eval_df.index  = indeces\n",
    "ae_eval_df.rename(columns=attack_index_dict, inplace=True)\n",
    "ae_eval_df_top = ae_eval_df.mean(axis = 1).sort_values(ascending=False)\n",
    "print(ae_eval_df_top.head(5))\n",
    "\n",
    "ae_best_detector = ae_ind_score[ae_ind_score[\"Model\"] == ae_eval_df_top.index[0]] #FIXME : Replace with the best detector\n",
    "# ae_best_detector = ae_ind_score[ae_ind_score[\"Model\"] == 'autoencoder_8_4_100']\n",
    "# Loading summary of perf_gen_dis_mean\n",
    "ae_ind_score.to_csv(f\"../{artifacts}/results_{version}/baseline_autoencoder.csv\")\n",
    "ae_best_detector.to_csv(f\"../{artifacts}/results_{version}/baseline_best_autoencoder.csv\")\n",
    "\n",
    "top_ae_models = []\n",
    "for model_id in ae_eval_df_top.index:\n",
    "    candidate = model_id.split(\"_\")[1:-1]\n",
    "    if candidate not in top_ae_models:\n",
    "        top_ae_models.append(candidate)\n",
    "top_ae_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_best_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top performing AE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize= (5, 3.5), sharex=True)\n",
    "for ax, (window, no_of_layers)  in zip(axes, top_ae_models[::-1]):\n",
    "    ae_ind_score_cut =  ae_ind_score[ae_ind_score['no_of_layers'] == int(no_of_layers)]\n",
    "    attack_slope = {}\n",
    "    for attack in ae_ind_score_cut[\"Attack\"].unique():\n",
    "        score = ae_ind_score_cut[ae_ind_score_cut[\"Attack\"] == attack]['mAUC'].diff().mean()\n",
    "        attack_slope[attack] = score\n",
    "    sorted_attacks = list(pd.Series(attack_slope).sort_values().index)\n",
    "    top_attacks = sorted_attacks[0:2] + sorted_attacks[-2:]\n",
    "    ae_ind_score_cut  = ae_ind_score_cut[ae_ind_score_cut['Attack'].isin(top_attacks)]\n",
    "    ae_ind_score_cut = ae_ind_score_cut.replace(attack_index_dict)\n",
    "\n",
    "    # TODO: Change mAUC to AUROC\n",
    "    sns.set_palette(\"PuBuGn_d\")\n",
    "    ax = sns.lineplot(ae_ind_score_cut, hue = 'Attack', style = 'Attack', linestyle = '--'\n",
    "                 , y = metric, x = 'epochs', linewidth = '0.75'\n",
    "                 , markersize = '8', dashes=False,  markers = True, palette=\"tab10\", ax = ax) #marker = 'p', markers = True, \n",
    "    ax.legend(title = \"Attack Index: \", bbox_to_anchor = (1.25,1.075), loc='upper center', fontsize = '10')\n",
    "    ax.set_title(f\"AE with $layers$={no_of_layers}\", fontsize = '10')\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([25, 50, 75, 100])\n",
    "    ax.set_ylim([0.0, 1.1])\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/multiple_attacks_{metric}_AE.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/multiple_attacks_{metric}_AE.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting other baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = 'artifacts'\n",
    "version = 'november_8_wisec_baseline_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ADS for all the discriminator\n",
    "bl_files = glob.glob(f'../{artifacts}/results_{version}/baseline*.json')\n",
    "bl_ind_score = pd.DataFrame([])\n",
    "bl_eval_df = pd.DataFrame([])\n",
    "for bl_file_name in bl_files:\n",
    "    print(bl_file_name)\n",
    "    model_id = \"_\".join( Path(bl_file_name).name.split(\"_\")[1:-1]).split(\".\")[0]\n",
    "    with open(f\"{bl_file_name}\", \"r\") as fp: \n",
    "        bl_score = json.load(fp)\n",
    "    bl_score = pd.DataFrame(bl_score).T\n",
    "    bl_score['mAUC'] = bl_score[['AUPRC', 'AUPRC']].mean(axis=1)\n",
    "    bl_eval_df[model_id] = bl_score['AUROC']\n",
    "    # bl_score['Model'] = model_id\n",
    "    bl_score['Model'] = bl_score['Detector']\n",
    "    bl_score['Base'] =  \"Baseline\"\n",
    "    bl_ind_score = pd.concat([bl_ind_score, bl_score], axis= 0, ignore_index=True)\n",
    "\n",
    "bl_ind_score_max = bl_ind_score.groupby('Attack').max()\n",
    "bl_ind_score_max['Attack'] = bl_ind_score_max.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_score['Model'] = bl_score['Detector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_ind_score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_baseline = bl_ind_score\n",
    "ae_baseline = ae_best_detector\n",
    "ae_baseline['Detector'] = \"AE\"\n",
    "ae_baseline['Base'] = \"AE\"\n",
    "ae_baseline = ae_baseline.drop(columns = [\"Model\"])\n",
    "ae_baseline['Model'] = \"AE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_ind_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ens_perf_ags_cut = ens_perf_ags[ens_perf_ags['Ensemble'] == ags_max_count]\n",
    "# ens_perf_ads_cut = ens_perf_ads[ens_perf_ads['Ensemble'] == ads_max_count]\n",
    "#FIXME : Replace with numbers\n",
    "ens_perf_ags_cut =  ens_perf_ads[ens_perf_ads['Ensemble'] == 10]\n",
    "ens_perf_ads_cut = ens_perf_ads[ens_perf_ads['Ensemble'] == 5]\n",
    "final_score = pd.concat([ens_perf_ags_cut, ens_perf_ads_cut, ae_baseline, other_baseline])\n",
    "final_score_cut = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(final_score_cut, x = 'Attack', y = 'AUROC', hue = 'Base', style = 'Base', palette=sns.mpl_palette(\"Set2\"),linewidth = '1', markersize= '10', markers=True)\n",
    "sns.lineplot(dis_ind_score_max, x = 'Attack', y = 'AUROC',  label = \"Max\", color = 'k', marker=\"p\")\n",
    "plt.xticks(rotation = 90)\n",
    "# plt.yticks(np.array(range(20, 110, 10))/100)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(final_score_cut, x = 'Attack', y = 'AUROC', hue = 'Base', palette=sns.mpl_palette(\"Set2\"),)\n",
    "sns.lineplot(dis_ind_score_max, x = 'Attack', y = 'AUROC',  label = \"Max\", color = 'k', marker=\"p\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_baseline = pd.DataFrame([])\n",
    "for metric in [\"AUROC\", \"AUPRC\"]:\n",
    "    df = final_score_cut[['Attack', metric, 'Model']]\n",
    "    df.rename(columns={metric: \"Score\"}, inplace=True)\n",
    "    df['Metric'] = metric\n",
    "    combined_baseline = pd.concat([combined_baseline, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2.25))\n",
    "# sns.swarmplot(combined_baseline, x = 'Model', y = \"Score\", hue=\"Metric\")\n",
    "ax = sns.boxplot(combined_baseline, x = 'Model', y = \"Score\", hue = \"Metric\", palette=sns.mpl_palette(\"Set2\"), width=.70, gap=0,)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"VeGAN with Baseline Models\")\n",
    "plt.ylabel(\"AUC Score\")\n",
    "# plt.title(\"Performance of VeGAN, compared to other MBDS baselines\")\n",
    "plt.legend(title = \"Score Type:\", ncols=1, bbox_to_anchor = (1.0, 1.0))\n",
    "plt.xticks(rotation = 0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/baseline_final_boxplot_All.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/baseline_final_boxplot_All.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Table\n",
    "data = final_score_cut[['Attack', 'AUROC', 'AUPRC', 'Model']]\n",
    "baseline_table = pd.DataFrame([])\n",
    "for attack in data['Attack'].unique():\n",
    "    for base in data['Model'].unique():\n",
    "        filter1 = data['Attack'] == attack\n",
    "        filter2 = data['Model'] == base\n",
    "        try:\n",
    "            baseline_table.loc[attack, base]= data.where(filter1 & filter2).dropna()['AUROC'].values[0]\n",
    "        except:\n",
    "            baseline_table.loc[attack, base]= 0 #TODO: Fix here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2.5))\n",
    "sns.boxplot(baseline_table)\n",
    "sns.swarmplot(baseline_table, color = 'k', alpha=0.75)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.grid()\n",
    "plt.xlabel(\"VeGAN with Baseline Model\")\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.title(\"Performance of VeGAN, compared to other MBDS baselines\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../artifacts/plots/baseline_final_boxplot_All.jpg\", dpi = 350)\n",
    "plt.savefig(f\"../artifacts/plots/baseline_final_boxplot_All.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_table.drop(columns=['$VehiGAN_{AGS}^{10}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix = baseline_table\n",
    "df_matrix.loc[\"Average\"] = df_matrix.mean(axis=0)\n",
    "# df_matrix ['$\\\\Delta_{AE}$'] =df_matrix[f'${sysname}_{{ADS}}^{{{ads_max_count}}}$'] - df_matrix['AE']\n",
    "df_matrix = np.round(df_matrix, 2)\n",
    "max_index = []\n",
    "for attak in df_matrix.values:\n",
    "    max_index.append(attak == max(attak))\n",
    "max_index = pd.DataFrame(max_index, columns=df_matrix.columns, index=df_matrix.index)\n",
    "df_matrix_copy = df_matrix.copy()\n",
    "for i in range(max_index.shape[0]):\n",
    "    for j in range(max_index.shape[1]):\n",
    "        if max_index.iloc[i,j] == True:\n",
    "            df_matrix_copy.iloc[i,j] = f\"\\\\textbf{{{df_matrix.iloc[i,j]}}}\"\n",
    "try:\n",
    "    print(df_matrix_copy.loc[cfg['selected_attacks'][1:] + ['Average']].astype(str).to_latex(index=True, formatters={\"name\": str.upper}, float_format=\"{:.3f}\".format))\n",
    "except:\n",
    "    print(df_matrix_copy.to_latex(index=True, formatters={\"name\": str.upper}, float_format=\"{:.3f}\".format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_1 = final_score_cut['Base'] == '$VehiGAN_{ADS}$'\n",
    "filter_2 = final_score_cut['Base'] == 'AE'\n",
    "final_score_test = final_score_cut.where(filter_1 | filter_2)[['Attack', 'AUROC', 'Base']].dropna()\n",
    "final_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((- final_score_test.iloc[0:35]['AUROC'].values + final_score_test.iloc[35:]['AUROC'].values) > 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_test.iloc[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_cut['Base'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
